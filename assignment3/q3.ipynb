{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Q3 (SVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import  Variable\n",
    "from torch import optim\n",
    "from torch.functional import F\n",
    "from classify_svhn import get_data_loader\n",
    "\n",
    "import GAN  # needed to allow the reload\n",
    "import vae\n",
    "import importlib\n",
    "importlib.reload(GAN)\n",
    "importlib.reload(vae)\n",
    "from GAN import Generator, Discriminator\n",
    "from vae import VAE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lr = 0.0001\n",
    "betas = (0, 0.9)\n",
    "batch_size = 64\n",
    "z_size = 100\n",
    "im_size = 32\n",
    "n_critic = 5\n",
    "num_epoch = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Running on cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "vae_model = VAE(type='SVHN').to(device)\n",
    "vae_optim = optim.Adam(vae_model.parameters(), lr=lr, betas=betas)\n",
    "    \n",
    "G = Generator(z_size).to(device)\n",
    "D = Discriminator(im_size, device).to(device)\n",
    "\n",
    "g_optim = optim.Adam(G.parameters(), lr=lr, betas=betas)\n",
    "d_optim = optim.Adam(D.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to svhn\\train_32x32.mat\n",
      "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to svhn\\test_32x32.mat\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def showImg(x):\n",
    "    x = x.permute(1, 2, 0)\n",
    "    plt.imshow((x.numpy() * 0.5) + 0.5)\n",
    "    \n",
    "train_loader, valid_loader, test_loader = get_data_loader(\"svhn\", batch_size)\n",
    "\n",
    "# Show an image\n",
    "real_sample, target = next(iter(train_loader))\n",
    "\n",
    "showImg(real_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader):\n",
    "    d_train_loss = 0\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        for data_idx, real_sample in enumerate(loader):\n",
    "            G.train()\n",
    "            D.train()\n",
    "            \n",
    "            step = epoch * len(loader) + data_idx + 1\n",
    "            \n",
    "            # Train more the dicriminator\n",
    "            d_optim.zero_grad()\n",
    "            g_optim.zero_grad()\n",
    "    \n",
    "            z = Variable(torch.randn(batch_size, z_size, device=device))\n",
    "            \n",
    "            fake_sample = G(z)\n",
    "            real_sample = real_sample.to(device)\n",
    "            \n",
    "            d_loss = D.loss(real_sample, fake_sample)\n",
    "            d_loss.backward()\n",
    "            d_optim.step()\n",
    "    \n",
    "            if step % n_critic == 0:\n",
    "                # Train the generator\n",
    "                d_optim.zero_grad()\n",
    "                g_optim.zero_grad()\n",
    "            \n",
    "                z = Variable(torch.randn(batch_size, z_size, device=device))\n",
    "            \n",
    "                fake_sample = G(z)\n",
    "                fake_result = D(fake_sample)\n",
    "                g_loss = G.loss(fake_result)\n",
    "                g_loss.backward()\n",
    "                g_optim.step()\n",
    "                \n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, d_train_loss / len(loader.dataset)))\n",
    "        \n",
    "        createSample(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSample(generator):\n",
    "    generator.eval()\n",
    "    \n",
    "    z = Variable(torch.randn(z_size, device=device))\n",
    "    im = generator(z)\n",
    "    showImg(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO_loss_function(recon_x, x, mu, logvar):\n",
    "    # ELBO: L(θ, φ; x) = -E_z~q_φ[log p_θ(x|z)] + D_KL(q_φ(z|x)||p(z))\n",
    "    # reconstruction loss + regularizer (forcing the encoder's output to stay close to a standard Normal distribution)\n",
    "    # BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    MSE = nn.MSELoss(reduction='sum')(recon_x, x)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    batch_size = x.shape[0]\n",
    "    \n",
    "    return (MSE + KLD) / batch_size\n",
    "\n",
    "\n",
    "def train_vae(model, num_epoch, train_loader, optimizer):\n",
    "    for epoch in range(num_epoch):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, y) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            flattened_data = data.view(-1, 3072)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(flattened_data)\n",
    "            loss = ELBO_loss_function(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.item()))\n",
    "\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "              epoch, train_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/65931 (0%)]\tLoss: 365.391602\n",
      "Train Epoch: 0 [6400/65931 (10%)]\tLoss: 335.915527\n",
      "Train Epoch: 0 [12800/65931 (19%)]\tLoss: 331.953491\n",
      "Train Epoch: 0 [19200/65931 (29%)]\tLoss: 298.137360\n",
      "Train Epoch: 0 [25600/65931 (39%)]\tLoss: 329.961792\n",
      "Train Epoch: 0 [32000/65931 (48%)]\tLoss: 309.068909\n",
      "Train Epoch: 0 [38400/65931 (58%)]\tLoss: 311.034851\n",
      "Train Epoch: 0 [44800/65931 (68%)]\tLoss: 309.366821\n",
      "Train Epoch: 0 [51200/65931 (78%)]\tLoss: 311.298859\n",
      "Train Epoch: 0 [57600/65931 (87%)]\tLoss: 314.777618\n",
      "Train Epoch: 0 [64000/65931 (97%)]\tLoss: 317.983704\n",
      "====> Epoch: 0 Average loss: 325.9786\n",
      "Train Epoch: 1 [0/65931 (0%)]\tLoss: 295.957458\n",
      "Train Epoch: 1 [6400/65931 (10%)]\tLoss: 319.064331\n",
      "Train Epoch: 1 [12800/65931 (19%)]\tLoss: 263.191925\n",
      "Train Epoch: 1 [19200/65931 (29%)]\tLoss: 284.173859\n",
      "Train Epoch: 1 [25600/65931 (39%)]\tLoss: 279.227448\n",
      "Train Epoch: 1 [32000/65931 (48%)]\tLoss: 282.745361\n",
      "Train Epoch: 1 [38400/65931 (58%)]\tLoss: 286.708435\n",
      "Train Epoch: 1 [44800/65931 (68%)]\tLoss: 303.413361\n",
      "Train Epoch: 1 [51200/65931 (78%)]\tLoss: 243.051666\n",
      "Train Epoch: 1 [57600/65931 (87%)]\tLoss: 262.370056\n",
      "Train Epoch: 1 [64000/65931 (97%)]\tLoss: 290.521912\n",
      "====> Epoch: 1 Average loss: 288.0987\n",
      "Train Epoch: 2 [0/65931 (0%)]\tLoss: 233.967255\n",
      "Train Epoch: 2 [6400/65931 (10%)]\tLoss: 287.370026\n",
      "Train Epoch: 2 [12800/65931 (19%)]\tLoss: 282.708954\n",
      "Train Epoch: 2 [19200/65931 (29%)]\tLoss: 273.409729\n",
      "Train Epoch: 2 [25600/65931 (39%)]\tLoss: 269.624084\n",
      "Train Epoch: 2 [32000/65931 (48%)]\tLoss: 254.837585\n",
      "Train Epoch: 2 [38400/65931 (58%)]\tLoss: 280.771149\n",
      "Train Epoch: 2 [44800/65931 (68%)]\tLoss: 286.117126\n",
      "Train Epoch: 2 [51200/65931 (78%)]\tLoss: 274.228119\n",
      "Train Epoch: 2 [57600/65931 (87%)]\tLoss: 257.415741\n",
      "Train Epoch: 2 [64000/65931 (97%)]\tLoss: 303.683044\n",
      "====> Epoch: 2 Average loss: 275.4261\n",
      "Train Epoch: 3 [0/65931 (0%)]\tLoss: 287.715363\n",
      "Train Epoch: 3 [6400/65931 (10%)]\tLoss: 245.857819\n",
      "Train Epoch: 3 [12800/65931 (19%)]\tLoss: 261.731934\n",
      "Train Epoch: 3 [19200/65931 (29%)]\tLoss: 246.826752\n",
      "Train Epoch: 3 [25600/65931 (39%)]\tLoss: 254.964203\n",
      "Train Epoch: 3 [32000/65931 (48%)]\tLoss: 259.262268\n",
      "Train Epoch: 3 [38400/65931 (58%)]\tLoss: 320.666656\n",
      "Train Epoch: 3 [44800/65931 (68%)]\tLoss: 259.452667\n",
      "Train Epoch: 3 [51200/65931 (78%)]\tLoss: 264.027313\n",
      "Train Epoch: 3 [57600/65931 (87%)]\tLoss: 241.878769\n",
      "Train Epoch: 3 [64000/65931 (97%)]\tLoss: 245.506012\n",
      "====> Epoch: 3 Average loss: 267.9045\n",
      "Train Epoch: 4 [0/65931 (0%)]\tLoss: 274.617737\n",
      "Train Epoch: 4 [6400/65931 (10%)]\tLoss: 252.607025\n",
      "Train Epoch: 4 [12800/65931 (19%)]\tLoss: 300.824219\n",
      "Train Epoch: 4 [19200/65931 (29%)]\tLoss: 266.069214\n",
      "Train Epoch: 4 [25600/65931 (39%)]\tLoss: 253.877380\n",
      "Train Epoch: 4 [32000/65931 (48%)]\tLoss: 253.872437\n",
      "Train Epoch: 4 [38400/65931 (58%)]\tLoss: 284.868713\n",
      "Train Epoch: 4 [44800/65931 (68%)]\tLoss: 269.401581\n",
      "Train Epoch: 4 [51200/65931 (78%)]\tLoss: 291.296173\n",
      "Train Epoch: 4 [57600/65931 (87%)]\tLoss: 260.178711\n",
      "Train Epoch: 4 [64000/65931 (97%)]\tLoss: 236.306030\n",
      "====> Epoch: 4 Average loss: 263.2100\n",
      "Train Epoch: 5 [0/65931 (0%)]\tLoss: 265.992554\n",
      "Train Epoch: 5 [6400/65931 (10%)]\tLoss: 252.432510\n",
      "Train Epoch: 5 [12800/65931 (19%)]\tLoss: 252.124481\n",
      "Train Epoch: 5 [19200/65931 (29%)]\tLoss: 273.120148\n",
      "Train Epoch: 5 [25600/65931 (39%)]\tLoss: 248.163910\n",
      "Train Epoch: 5 [32000/65931 (48%)]\tLoss: 251.657715\n",
      "Train Epoch: 5 [38400/65931 (58%)]\tLoss: 262.282379\n",
      "Train Epoch: 5 [44800/65931 (68%)]\tLoss: 239.169312\n",
      "Train Epoch: 5 [51200/65931 (78%)]\tLoss: 291.176575\n",
      "Train Epoch: 5 [57600/65931 (87%)]\tLoss: 238.881348\n",
      "Train Epoch: 5 [64000/65931 (97%)]\tLoss: 240.946365\n",
      "====> Epoch: 5 Average loss: 260.1973\n",
      "Train Epoch: 6 [0/65931 (0%)]\tLoss: 243.668503\n",
      "Train Epoch: 6 [6400/65931 (10%)]\tLoss: 217.312378\n",
      "Train Epoch: 6 [12800/65931 (19%)]\tLoss: 265.939636\n",
      "Train Epoch: 6 [19200/65931 (29%)]\tLoss: 278.507294\n",
      "Train Epoch: 6 [25600/65931 (39%)]\tLoss: 258.522552\n",
      "Train Epoch: 6 [32000/65931 (48%)]\tLoss: 330.231873\n",
      "Train Epoch: 6 [38400/65931 (58%)]\tLoss: 251.164108\n",
      "Train Epoch: 6 [44800/65931 (68%)]\tLoss: 221.906784\n",
      "Train Epoch: 6 [51200/65931 (78%)]\tLoss: 284.246063\n",
      "Train Epoch: 6 [57600/65931 (87%)]\tLoss: 261.433044\n",
      "Train Epoch: 6 [64000/65931 (97%)]\tLoss: 256.483887\n",
      "====> Epoch: 6 Average loss: 258.1882\n",
      "Train Epoch: 7 [0/65931 (0%)]\tLoss: 283.032074\n",
      "Train Epoch: 7 [6400/65931 (10%)]\tLoss: 237.194412\n",
      "Train Epoch: 7 [12800/65931 (19%)]\tLoss: 299.258820\n",
      "Train Epoch: 7 [19200/65931 (29%)]\tLoss: 276.402283\n",
      "Train Epoch: 7 [25600/65931 (39%)]\tLoss: 254.242203\n",
      "Train Epoch: 7 [32000/65931 (48%)]\tLoss: 293.374512\n",
      "Train Epoch: 7 [38400/65931 (58%)]\tLoss: 268.461212\n",
      "Train Epoch: 7 [44800/65931 (68%)]\tLoss: 223.036362\n",
      "Train Epoch: 7 [51200/65931 (78%)]\tLoss: 223.352783\n",
      "Train Epoch: 7 [57600/65931 (87%)]\tLoss: 295.834839\n",
      "Train Epoch: 7 [64000/65931 (97%)]\tLoss: 273.363098\n",
      "====> Epoch: 7 Average loss: 255.9243\n",
      "Train Epoch: 8 [0/65931 (0%)]\tLoss: 301.632050\n",
      "Train Epoch: 8 [6400/65931 (10%)]\tLoss: 227.599472\n",
      "Train Epoch: 8 [12800/65931 (19%)]\tLoss: 297.749664\n",
      "Train Epoch: 8 [19200/65931 (29%)]\tLoss: 245.444290\n",
      "Train Epoch: 8 [25600/65931 (39%)]\tLoss: 268.636200\n",
      "Train Epoch: 8 [32000/65931 (48%)]\tLoss: 273.390991\n",
      "Train Epoch: 8 [38400/65931 (58%)]\tLoss: 220.967636\n",
      "Train Epoch: 8 [44800/65931 (68%)]\tLoss: 268.239746\n",
      "Train Epoch: 8 [51200/65931 (78%)]\tLoss: 207.406082\n",
      "Train Epoch: 8 [57600/65931 (87%)]\tLoss: 269.449280\n",
      "Train Epoch: 8 [64000/65931 (97%)]\tLoss: 212.694077\n",
      "====> Epoch: 8 Average loss: 254.4615\n",
      "Train Epoch: 9 [0/65931 (0%)]\tLoss: 247.066040\n",
      "Train Epoch: 9 [6400/65931 (10%)]\tLoss: 288.777985\n",
      "Train Epoch: 9 [12800/65931 (19%)]\tLoss: 233.712448\n",
      "Train Epoch: 9 [19200/65931 (29%)]\tLoss: 266.083862\n",
      "Train Epoch: 9 [25600/65931 (39%)]\tLoss: 271.432434\n",
      "Train Epoch: 9 [32000/65931 (48%)]\tLoss: 285.670166\n",
      "Train Epoch: 9 [38400/65931 (58%)]\tLoss: 259.831848\n",
      "Train Epoch: 9 [44800/65931 (68%)]\tLoss: 224.622177\n",
      "Train Epoch: 9 [51200/65931 (78%)]\tLoss: 219.157791\n",
      "Train Epoch: 9 [57600/65931 (87%)]\tLoss: 237.796631\n",
      "Train Epoch: 9 [64000/65931 (97%)]\tLoss: 239.143555\n",
      "====> Epoch: 9 Average loss: 252.8522\n"
     ]
    }
   ],
   "source": [
    "train_vae(vae_model, num_epoch, train_loader, vae_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
