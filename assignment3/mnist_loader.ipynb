{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import utils\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn.modules import upsampling\n",
    "from torch.functional import F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset_location, batch_size):\n",
    "    URL = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n",
    "    # start processing\n",
    "    def lines_to_np_array(lines):\n",
    "        return np.array([[int(i) for i in line.split()] for line in lines])\n",
    "    splitdata = []\n",
    "    for splitname in [\"train\", \"valid\", \"test\"]:\n",
    "        filename = \"binarized_mnist_%s.amat\" % splitname\n",
    "        filepath = os.path.join(dataset_location, filename)\n",
    "        if not os.path.isfile(filepath):\n",
    "            utils.download_url(URL + filename, dataset_location, filename, None)\n",
    "        with open(filepath) as f:\n",
    "            lines = f.readlines()\n",
    "        x = lines_to_np_array(lines).astype('float32')\n",
    "        x = x.reshape(x.shape[0], 1, 28, 28)\n",
    "        # pytorch data loader\n",
    "        dataset = data_utils.TensorDataset(torch.from_numpy(x))\n",
    "        dataset_loader = data_utils.DataLoader(x, batch_size=batch_size, shuffle=splitname == \"train\")\n",
    "        splitdata.append(dataset_loader)\n",
    "    return splitdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = get_data_loader(\"binarized_mnist\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "for x in train_loader:\n",
    "    plt.imshow(x[0, 0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vae  # needed to allow the reload\n",
    "import importlib\n",
    "importlib.reload(vae)  # forces a reloading of the module (because jupyter notebook will not reload it after it has been modified)\n",
    "from vae import VAE\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "\n",
    "def ELBO_loss_function(recon_x, x, mu, logvar):\n",
    "    # ELBO: L(θ, φ; x) = -E_z~q_φ[log p_θ(x|z)] + D_KL(q_φ(z|x)||p(z))\n",
    "    # reconstruction loss + regularizer (forcing the encoder's output to stay close to a standard Normal distribution)\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    batch_size = x.shape[0]\n",
    "    \n",
    "    return (BCE + KLD) / batch_size\n",
    "\n",
    "\n",
    "def train_vae(epoch, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = ELBO_loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item()))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader)))\n",
    "\n",
    "    \n",
    "def test_vae(epoch, test_loader):\n",
    "    model.eval()\n",
    "    nb_examples = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            batch_size = data.shape[0]\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += ELBO_loss_function(recon_batch, data, mu, logvar).item() * batch_size\n",
    "            nb_examples += batch_size\n",
    "#             if i == 0:\n",
    "#                 n = min(data.size(0), 8)\n",
    "#                 comparison = torch.cat([data[:n], recon_batch.view(64, 1, 28, 28)[:n]])\n",
    "#                 save_image(comparison.cpu(), 'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= nb_examples\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 549.177673\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 221.149353\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 184.490433\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 164.763855\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 165.892441\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 146.071671\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 147.616241\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 137.116226\n",
      "====> Epoch: 0 Average loss: 180.8418\n",
      "====> Test set loss: 137.2409\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 141.433823\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 136.259674\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 134.421616\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 137.932571\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 123.571335\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 127.106209\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 116.417206\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 122.746544\n",
      "====> Epoch: 1 Average loss: 125.6401\n",
      "====> Test set loss: 117.0890\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 117.577209\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 109.952339\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 108.885574\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 115.078888\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 109.847580\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 113.006989\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 114.897301\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 111.460098\n",
      "====> Epoch: 2 Average loss: 112.5627\n",
      "====> Test set loss: 109.3776\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 109.777664\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 108.857079\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 108.785500\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 109.254166\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 101.814713\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 110.684517\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 105.543488\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 105.138054\n",
      "====> Epoch: 3 Average loss: 106.8934\n",
      "====> Test set loss: 105.1419\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 100.663544\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 100.421616\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 103.305099\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 101.949944\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 106.984772\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 102.193817\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 103.801086\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 105.464699\n",
      "====> Epoch: 4 Average loss: 103.6274\n",
      "====> Test set loss: 102.8306\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 101.013596\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 97.850807\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 107.180344\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 102.898056\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 110.721596\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 101.101791\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 102.843460\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 103.552353\n",
      "====> Epoch: 5 Average loss: 101.6604\n",
      "====> Test set loss: 101.8534\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 99.842102\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 101.363594\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 101.731995\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 92.413788\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 100.079315\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 102.581543\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 100.725868\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 102.184738\n",
      "====> Epoch: 6 Average loss: 100.2838\n",
      "====> Test set loss: 99.9992\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 101.289673\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 99.467194\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 108.854935\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 98.289627\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 102.867340\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 98.197540\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 98.160873\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 98.202850\n",
      "====> Epoch: 7 Average loss: 99.2454\n",
      "====> Test set loss: 99.1973\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 97.582291\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 100.388443\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 102.542694\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 96.144775\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 102.507416\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 96.889023\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 101.106308\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 96.768829\n",
      "====> Epoch: 8 Average loss: 98.3363\n",
      "====> Test set loss: 98.5902\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 99.658081\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 102.368561\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 104.617996\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 101.716843\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 102.475891\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 100.374527\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 96.506424\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 91.834373\n",
      "====> Epoch: 9 Average loss: 97.6668\n",
      "====> Test set loss: 97.7399\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 101.754578\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 97.444290\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 96.389023\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 102.047623\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 104.031494\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 104.233139\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 97.461243\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 97.058563\n",
      "====> Epoch: 10 Average loss: 97.0211\n",
      "====> Test set loss: 96.9739\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 91.530937\n",
      "Train Epoch: 11 [6400/50000 (13%)]\tLoss: 95.340050\n",
      "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 98.102646\n",
      "Train Epoch: 11 [19200/50000 (38%)]\tLoss: 95.188980\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 96.124725\n",
      "Train Epoch: 11 [32000/50000 (64%)]\tLoss: 94.289932\n",
      "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 92.784790\n",
      "Train Epoch: 11 [44800/50000 (90%)]\tLoss: 94.366180\n",
      "====> Epoch: 11 Average loss: 96.5248\n",
      "====> Test set loss: 96.8461\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 94.380142\n",
      "Train Epoch: 12 [6400/50000 (13%)]\tLoss: 95.102531\n",
      "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 101.477142\n",
      "Train Epoch: 12 [19200/50000 (38%)]\tLoss: 97.269356\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 91.724884\n",
      "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 95.955208\n",
      "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 98.364746\n",
      "Train Epoch: 12 [44800/50000 (90%)]\tLoss: 94.519653\n",
      "====> Epoch: 12 Average loss: 96.0099\n",
      "====> Test set loss: 96.2522\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 98.822800\n",
      "Train Epoch: 13 [6400/50000 (13%)]\tLoss: 90.594612\n",
      "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 95.393944\n",
      "Train Epoch: 13 [19200/50000 (38%)]\tLoss: 90.896820\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 91.834290\n",
      "Train Epoch: 13 [32000/50000 (64%)]\tLoss: 93.057938\n",
      "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 96.726372\n",
      "Train Epoch: 13 [44800/50000 (90%)]\tLoss: 91.324745\n",
      "====> Epoch: 13 Average loss: 95.5364\n",
      "====> Test set loss: 96.0421\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 90.067001\n",
      "Train Epoch: 14 [6400/50000 (13%)]\tLoss: 95.613388\n",
      "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 93.507645\n",
      "Train Epoch: 14 [19200/50000 (38%)]\tLoss: 94.342255\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 90.237907\n",
      "Train Epoch: 14 [32000/50000 (64%)]\tLoss: 96.145187\n",
      "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 99.102325\n",
      "Train Epoch: 14 [44800/50000 (90%)]\tLoss: 96.119957\n",
      "====> Epoch: 14 Average loss: 95.2057\n",
      "====> Test set loss: 95.5250\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 93.029419\n",
      "Train Epoch: 15 [6400/50000 (13%)]\tLoss: 94.693123\n",
      "Train Epoch: 15 [12800/50000 (26%)]\tLoss: 94.916336\n",
      "Train Epoch: 15 [19200/50000 (38%)]\tLoss: 98.539711\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 91.419083\n",
      "Train Epoch: 15 [32000/50000 (64%)]\tLoss: 96.450897\n",
      "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 94.971054\n",
      "Train Epoch: 15 [44800/50000 (90%)]\tLoss: 96.150482\n",
      "====> Epoch: 15 Average loss: 94.7988\n",
      "====> Test set loss: 95.2293\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 100.033318\n",
      "Train Epoch: 16 [6400/50000 (13%)]\tLoss: 92.304199\n",
      "Train Epoch: 16 [12800/50000 (26%)]\tLoss: 93.988480\n",
      "Train Epoch: 16 [19200/50000 (38%)]\tLoss: 97.394806\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 100.796921\n",
      "Train Epoch: 16 [32000/50000 (64%)]\tLoss: 97.502518\n",
      "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 90.094810\n",
      "Train Epoch: 16 [44800/50000 (90%)]\tLoss: 98.751633\n",
      "====> Epoch: 16 Average loss: 94.5158\n",
      "====> Test set loss: 95.0400\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 94.858894\n",
      "Train Epoch: 17 [6400/50000 (13%)]\tLoss: 92.867844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [12800/50000 (26%)]\tLoss: 102.144516\n",
      "Train Epoch: 17 [19200/50000 (38%)]\tLoss: 93.785812\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 95.418083\n",
      "Train Epoch: 17 [32000/50000 (64%)]\tLoss: 91.882706\n",
      "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 94.817940\n",
      "Train Epoch: 17 [44800/50000 (90%)]\tLoss: 91.646324\n",
      "====> Epoch: 17 Average loss: 94.2574\n",
      "====> Test set loss: 94.8378\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 96.356934\n",
      "Train Epoch: 18 [6400/50000 (13%)]\tLoss: 92.918198\n",
      "Train Epoch: 18 [12800/50000 (26%)]\tLoss: 92.706802\n",
      "Train Epoch: 18 [19200/50000 (38%)]\tLoss: 100.813385\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 96.421371\n",
      "Train Epoch: 18 [32000/50000 (64%)]\tLoss: 97.982330\n",
      "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 101.981293\n",
      "Train Epoch: 18 [44800/50000 (90%)]\tLoss: 93.683815\n",
      "====> Epoch: 18 Average loss: 94.0198\n",
      "====> Test set loss: 94.5863\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 93.741051\n",
      "Train Epoch: 19 [6400/50000 (13%)]\tLoss: 90.401367\n",
      "Train Epoch: 19 [12800/50000 (26%)]\tLoss: 90.165665\n",
      "Train Epoch: 19 [19200/50000 (38%)]\tLoss: 92.150665\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 94.074242\n",
      "Train Epoch: 19 [32000/50000 (64%)]\tLoss: 95.250122\n",
      "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 95.265427\n",
      "Train Epoch: 19 [44800/50000 (90%)]\tLoss: 89.637924\n",
      "====> Epoch: 19 Average loss: 93.6923\n",
      "====> Test set loss: 94.1354\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    train_vae(epoch, train_loader)\n",
    "    test_vae(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'vae_elbo.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu torch.Size([64, 200, 100])\n",
      "eps torch.Size([64, 200, 100])\n",
      "std torch.Size([64, 200, 100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [64, 256, 5, 5], but got 3-dimensional input of size [256, 1, 1] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-90d06e1f140b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mloglikelihood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_loglikelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloglikelihood\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-90d06e1f140b>\u001b[0m in \u001b[0;36mevaluate_loglikelihood\u001b[1;34m(model, x, K)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"std\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmu\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0meps\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportance_sampling_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-90d06e1f140b>\u001b[0m in \u001b[0;36mimportance_sampling_loss_function\u001b[1;34m(model, x, z)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0msum_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mz_ik\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mz_i\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mlog_pxz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_ik\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mlog_pz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgaussian_sample_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_ik\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mlog_qzx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgaussian_sample_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_ik\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\polymtl\\deep_learning\\IFT6135H19_assignment\\assignment3\\vae.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder_activation1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 256, 5, 5], but got 3-dimensional input of size [256, 1, 1] instead"
     ]
    }
   ],
   "source": [
    "# x has shape (M, D)\n",
    "# z has shape (M, K, D)\n",
    "# Where M is mini-batch size, D is the dimension of the input and K is importance sampling count\n",
    "def importance_sampling_loss_function(model, x, z):\n",
    "    # log sum_k exp (log p(x|z) + log p(z) - log q(z|x)) - log k\n",
    "    losses = []\n",
    "    K = z.shape[1]\n",
    "    log_k = np.log(K)\n",
    "    for x_i, z_i in zip(x, z):\n",
    "        mu, logvar = model.encode(x)\n",
    "        sum_k = 0\n",
    "        for z_ik in z_i:\n",
    "            log_pxz = torch.log(model.decode(z_ik))\n",
    "            log_pz = torch.log(gaussian_sample_probability(z_ik, 0, 1))\n",
    "            log_qzx = torch.log(gaussian_sample_probability(z_ik, mu, logvar))\n",
    "            sum_k += torch.exp(log_pxz + log_pz - log_qzx)\n",
    "        L = torch.log(sum_k) - log_k\n",
    "        losses.append(L)\n",
    "    return losses\n",
    "    \n",
    "\n",
    "def gaussian_sample_probabity(z, mu, logvar):\n",
    "    var = torch.exp(logvar)\n",
    "    coef = 1 / torch.sqrt(2 * torch.np.math.pi * var)\n",
    "    exponential = torch.exp(-(z - mu).pow(2) / 2 * var)\n",
    "    return exponential * coef\n",
    "\n",
    "\n",
    "def evaluate_loglikelihood(model, x, K=200):\n",
    "    mu, logvar = model.encode(x)\n",
    "    mu = torch.unsqueeze(mu, 1)\n",
    "    mu = mu.repeat(1, K, 1)\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    std = torch.unsqueeze(std, 1)\n",
    "    std = std.repeat(1, K, 1)\n",
    "    eps = torch.randn_like(std)\n",
    "    print(\"mu\", mu.size())\n",
    "    print(\"eps\", eps.size())\n",
    "    print(\"std\", std.size())\n",
    "    z = mu + eps * std\n",
    "    losses = importance_sampling_loss_function(model, x, z)\n",
    "    return losses.mean()\n",
    "\n",
    "for i, data in enumerate(test_loader):\n",
    "    data = data.to(device)\n",
    "    loglikelihood = evaluate_loglikelihood(model, data)\n",
    "    print(loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
