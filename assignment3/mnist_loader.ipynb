{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import utils\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn.modules import upsampling\n",
    "from torch.functional import F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset_location, batch_size):\n",
    "    URL = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n",
    "    # start processing\n",
    "    def lines_to_np_array(lines):\n",
    "        return np.array([[int(i) for i in line.split()] for line in lines])\n",
    "    splitdata = []\n",
    "    for splitname in [\"train\", \"valid\", \"test\"]:\n",
    "        filename = \"binarized_mnist_%s.amat\" % splitname\n",
    "        filepath = os.path.join(dataset_location, filename)\n",
    "        if not os.path.isfile(filepath):\n",
    "            utils.download_url(URL + filename, dataset_location, filename, None)\n",
    "        with open(filepath) as f:\n",
    "            lines = f.readlines()\n",
    "        x = lines_to_np_array(lines).astype('float32')\n",
    "        x = x.reshape(x.shape[0], 1, 28, 28)\n",
    "        # pytorch data loader\n",
    "        dataset = data_utils.TensorDataset(torch.from_numpy(x))\n",
    "        dataset_loader = data_utils.DataLoader(x, batch_size=batch_size, shuffle=splitname == \"train\")\n",
    "        splitdata.append(dataset_loader)\n",
    "    return splitdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = get_data_loader(\"binarized_mnist\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "for x in train_loader:\n",
    "    plt.imshow(x[0, 0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vae  # needed to allow the reload\n",
    "import importlib\n",
    "importlib.reload(vae)  # forces a reloading of the module (because jupyter notebook will not reload it after it has been modified)\n",
    "from vae import VAE\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # ELBO: L(θ, φ; x) = -E_z~q_φ[log p_θ(x|z)] + D_KL(q_φ(z|x)||p(z))\n",
    "    # reconstruction loss + regularizer (forcing the encoder's output to stay close to a standard Normal distribution)\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train_vae(epoch, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "    \n",
    "def test_vae(epoch, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 563.821533\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 218.233353\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 189.457657\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 167.045456\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 161.924500\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 155.691879\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 156.223328\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 146.818283\n",
      "====> Epoch: 0 Average loss: 183.8234\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 137.664185\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 141.826996\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 132.653320\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 121.459938\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 125.207146\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 121.468819\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 126.338188\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 114.920319\n",
      "====> Epoch: 1 Average loss: 126.0084\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 113.005142\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 117.441986\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 112.797882\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 112.201462\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 109.975693\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 111.043098\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 114.527618\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 112.077667\n",
      "====> Epoch: 2 Average loss: 113.3607\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 113.170448\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 111.333885\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 104.549950\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 111.858612\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 108.481911\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 98.700592\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 101.952774\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 105.555000\n",
      "====> Epoch: 3 Average loss: 107.5995\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 105.246536\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 105.932289\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 101.246262\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 101.969437\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 101.166687\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 104.697609\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 106.848724\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 105.383545\n",
      "====> Epoch: 4 Average loss: 104.2346\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 103.389984\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 102.816475\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 95.567513\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 106.288391\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 102.542679\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 99.858040\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 104.428818\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 100.546158\n",
      "====> Epoch: 5 Average loss: 102.0910\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 97.903458\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 102.333977\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 100.011932\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 99.757736\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 98.226044\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 95.417992\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 105.564087\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 102.200989\n",
      "====> Epoch: 6 Average loss: 100.6604\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 98.918083\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 101.594299\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 100.869186\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 100.588409\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 101.762833\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 94.958084\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 94.659950\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 103.584656\n",
      "====> Epoch: 7 Average loss: 99.4481\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 106.796219\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 100.303444\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 98.174355\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 98.841652\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 95.263885\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 97.735672\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 90.663895\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 97.065887\n",
      "====> Epoch: 8 Average loss: 98.5278\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 96.552574\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 92.876038\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 94.479485\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 97.163300\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 96.623795\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 102.510109\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 96.386642\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 96.056938\n",
      "====> Epoch: 9 Average loss: 97.7341\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 95.915512\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 95.536728\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 93.151268\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 97.566589\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 95.480751\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 101.026047\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 98.610725\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 96.468712\n",
      "====> Epoch: 10 Average loss: 97.0357\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 102.489288\n",
      "Train Epoch: 11 [6400/50000 (13%)]\tLoss: 99.029297\n",
      "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 102.705856\n",
      "Train Epoch: 11 [19200/50000 (38%)]\tLoss: 102.606506\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 94.596420\n",
      "Train Epoch: 11 [32000/50000 (64%)]\tLoss: 95.416222\n",
      "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 99.742310\n",
      "Train Epoch: 11 [44800/50000 (90%)]\tLoss: 94.620880\n",
      "====> Epoch: 11 Average loss: 96.5801\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 96.331985\n",
      "Train Epoch: 12 [6400/50000 (13%)]\tLoss: 95.955597\n",
      "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 94.568092\n",
      "Train Epoch: 12 [19200/50000 (38%)]\tLoss: 98.954620\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 95.344765\n",
      "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 101.453011\n",
      "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 97.811249\n",
      "Train Epoch: 12 [44800/50000 (90%)]\tLoss: 94.559441\n",
      "====> Epoch: 12 Average loss: 96.0703\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 100.690826\n",
      "Train Epoch: 13 [6400/50000 (13%)]\tLoss: 99.367348\n",
      "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 96.525406\n",
      "Train Epoch: 13 [19200/50000 (38%)]\tLoss: 94.065552\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 95.777252\n",
      "Train Epoch: 13 [32000/50000 (64%)]\tLoss: 97.923363\n",
      "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 97.697418\n",
      "Train Epoch: 13 [44800/50000 (90%)]\tLoss: 98.863510\n",
      "====> Epoch: 13 Average loss: 95.6170\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 98.075241\n",
      "Train Epoch: 14 [6400/50000 (13%)]\tLoss: 90.777634\n",
      "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 92.790009\n",
      "Train Epoch: 14 [19200/50000 (38%)]\tLoss: 90.308128\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 93.391487\n",
      "Train Epoch: 14 [32000/50000 (64%)]\tLoss: 103.111076\n",
      "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 99.129684\n",
      "Train Epoch: 14 [44800/50000 (90%)]\tLoss: 93.129448\n",
      "====> Epoch: 14 Average loss: 95.2030\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 97.369888\n",
      "Train Epoch: 15 [6400/50000 (13%)]\tLoss: 99.295639\n",
      "Train Epoch: 15 [12800/50000 (26%)]\tLoss: 89.692780\n",
      "Train Epoch: 15 [19200/50000 (38%)]\tLoss: 92.549286\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 99.175079\n",
      "Train Epoch: 15 [32000/50000 (64%)]\tLoss: 92.987915\n",
      "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 98.453110\n",
      "Train Epoch: 15 [44800/50000 (90%)]\tLoss: 93.473076\n",
      "====> Epoch: 15 Average loss: 94.9117\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 97.598457\n",
      "Train Epoch: 16 [6400/50000 (13%)]\tLoss: 92.699318\n",
      "Train Epoch: 16 [12800/50000 (26%)]\tLoss: 95.205902\n",
      "Train Epoch: 16 [19200/50000 (38%)]\tLoss: 95.989677\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 99.159485\n",
      "Train Epoch: 16 [32000/50000 (64%)]\tLoss: 93.778503\n",
      "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 94.677719\n",
      "Train Epoch: 16 [44800/50000 (90%)]\tLoss: 90.499603\n",
      "====> Epoch: 16 Average loss: 94.5928\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 94.015915\n",
      "Train Epoch: 17 [6400/50000 (13%)]\tLoss: 93.580605\n",
      "Train Epoch: 17 [12800/50000 (26%)]\tLoss: 87.099442\n",
      "Train Epoch: 17 [19200/50000 (38%)]\tLoss: 89.590874\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 97.148880\n",
      "Train Epoch: 17 [32000/50000 (64%)]\tLoss: 99.964661\n",
      "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 93.977753\n",
      "Train Epoch: 17 [44800/50000 (90%)]\tLoss: 93.216240\n",
      "====> Epoch: 17 Average loss: 94.2550\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 87.381775\n",
      "Train Epoch: 18 [6400/50000 (13%)]\tLoss: 93.146233\n",
      "Train Epoch: 18 [12800/50000 (26%)]\tLoss: 92.908020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 18 [19200/50000 (38%)]\tLoss: 97.718872\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 95.081802\n",
      "Train Epoch: 18 [32000/50000 (64%)]\tLoss: 93.556259\n",
      "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 92.734146\n",
      "Train Epoch: 18 [44800/50000 (90%)]\tLoss: 93.239227\n",
      "====> Epoch: 18 Average loss: 94.0313\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 93.091507\n",
      "Train Epoch: 19 [6400/50000 (13%)]\tLoss: 90.007622\n",
      "Train Epoch: 19 [12800/50000 (26%)]\tLoss: 96.397949\n",
      "Train Epoch: 19 [19200/50000 (38%)]\tLoss: 93.749725\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 92.763412\n",
      "Train Epoch: 19 [32000/50000 (64%)]\tLoss: 92.896576\n",
      "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 93.889969\n",
      "Train Epoch: 19 [44800/50000 (90%)]\tLoss: 94.129547\n",
      "====> Epoch: 19 Average loss: 93.7822\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    train_vae(epoch, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
