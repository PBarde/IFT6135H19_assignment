{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import utils\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn.modules import upsampling\n",
    "from torch.functional import F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset_location, batch_size):\n",
    "    URL = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n",
    "    # start processing\n",
    "    def lines_to_np_array(lines):\n",
    "        return np.array([[int(i) for i in line.split()] for line in lines])\n",
    "    splitdata = []\n",
    "    for splitname in [\"train\", \"valid\", \"test\"]:\n",
    "        filename = \"binarized_mnist_%s.amat\" % splitname\n",
    "        filepath = os.path.join(dataset_location, filename)\n",
    "        if not os.path.isfile(filepath):\n",
    "            utils.download_url(URL + filename, dataset_location, filename, None)\n",
    "        with open(filepath) as f:\n",
    "            lines = f.readlines()\n",
    "        x = lines_to_np_array(lines).astype('float32')\n",
    "        x = x.reshape(x.shape[0], 1, 28, 28)\n",
    "        # pytorch data loader\n",
    "        dataset = data_utils.TensorDataset(torch.from_numpy(x))\n",
    "        dataset_loader = data_utils.DataLoader(x, batch_size=batch_size, shuffle=splitname == \"train\")\n",
    "        splitdata.append(dataset_loader)\n",
    "    return splitdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = get_data_loader(\"binarized_mnist\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "for x in train_loader:\n",
    "    plt.imshow(x[0, 0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vae  # needed to allow the reload\n",
    "import importlib\n",
    "importlib.reload(vae)  # forces a reloading of the module (because jupyter notebook will not reload it after it has been modified)\n",
    "from vae import VAE\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "\n",
    "def ELBO_loss_function(recon_x, x, mu, logvar):\n",
    "    # ELBO: L(θ, φ; x) = -E_z~q_φ[log p_θ(x|z)] + D_KL(q_φ(z|x)||p(z))\n",
    "    # reconstruction loss + regularizer (forcing the encoder's output to stay close to a standard Normal distribution)\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    batch_size = x.shape[0]\n",
    "    \n",
    "    return (BCE + KLD) / batch_size\n",
    "\n",
    "\n",
    "def train_vae(epoch, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = ELBO_loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item()))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader)))\n",
    "\n",
    "    \n",
    "def test_vae(epoch, test_loader):\n",
    "    model.eval()\n",
    "    nb_examples = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            batch_size = data.shape[0]\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += ELBO_loss_function(recon_batch, data, mu, logvar).item() * batch_size\n",
    "            nb_examples += batch_size\n",
    "#             if i == 0:\n",
    "#                 n = min(data.size(0), 8)\n",
    "#                 comparison = torch.cat([data[:n], recon_batch.view(64, 1, 28, 28)[:n]])\n",
    "#                 save_image(comparison.cpu(), 'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= nb_examples\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 549.177673\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 221.149353\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 184.490433\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 164.763855\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 165.892441\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 146.071671\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 147.616241\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 137.116226\n",
      "====> Epoch: 0 Average loss: 180.8418\n",
      "====> Test set loss: 137.2409\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 141.433823\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 136.259674\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 134.421616\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 137.932571\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 123.571335\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 127.106209\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 116.417206\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 122.746544\n",
      "====> Epoch: 1 Average loss: 125.6401\n",
      "====> Test set loss: 117.0890\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 117.577209\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 109.952339\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 108.885574\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 115.078888\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 109.847580\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 113.006989\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 114.897301\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 111.460098\n",
      "====> Epoch: 2 Average loss: 112.5627\n",
      "====> Test set loss: 109.3776\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 109.777664\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 108.857079\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 108.785500\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 109.254166\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 101.814713\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 110.684517\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 105.543488\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 105.138054\n",
      "====> Epoch: 3 Average loss: 106.8934\n",
      "====> Test set loss: 105.1419\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 100.663544\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 100.421616\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 103.305099\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 101.949944\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 106.984772\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 102.193817\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 103.801086\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 105.464699\n",
      "====> Epoch: 4 Average loss: 103.6274\n",
      "====> Test set loss: 102.8306\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 101.013596\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 97.850807\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 107.180344\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 102.898056\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 110.721596\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 101.101791\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 102.843460\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 103.552353\n",
      "====> Epoch: 5 Average loss: 101.6604\n",
      "====> Test set loss: 101.8534\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 99.842102\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 101.363594\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 101.731995\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 92.413788\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 100.079315\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 102.581543\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 100.725868\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 102.184738\n",
      "====> Epoch: 6 Average loss: 100.2838\n",
      "====> Test set loss: 99.9992\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 101.289673\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 99.467194\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 108.854935\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 98.289627\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 102.867340\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 98.197540\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 98.160873\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 98.202850\n",
      "====> Epoch: 7 Average loss: 99.2454\n",
      "====> Test set loss: 99.1973\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 97.582291\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 100.388443\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 102.542694\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 96.144775\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 102.507416\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 96.889023\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 101.106308\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 96.768829\n",
      "====> Epoch: 8 Average loss: 98.3363\n",
      "====> Test set loss: 98.5902\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 99.658081\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 102.368561\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 104.617996\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 101.716843\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 102.475891\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 100.374527\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 96.506424\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 91.834373\n",
      "====> Epoch: 9 Average loss: 97.6668\n",
      "====> Test set loss: 97.7399\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 101.754578\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 97.444290\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 96.389023\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 102.047623\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 104.031494\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 104.233139\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 97.461243\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 97.058563\n",
      "====> Epoch: 10 Average loss: 97.0211\n",
      "====> Test set loss: 96.9739\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 91.530937\n",
      "Train Epoch: 11 [6400/50000 (13%)]\tLoss: 95.340050\n",
      "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 98.102646\n",
      "Train Epoch: 11 [19200/50000 (38%)]\tLoss: 95.188980\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 96.124725\n",
      "Train Epoch: 11 [32000/50000 (64%)]\tLoss: 94.289932\n",
      "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 92.784790\n",
      "Train Epoch: 11 [44800/50000 (90%)]\tLoss: 94.366180\n",
      "====> Epoch: 11 Average loss: 96.5248\n",
      "====> Test set loss: 96.8461\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 94.380142\n",
      "Train Epoch: 12 [6400/50000 (13%)]\tLoss: 95.102531\n",
      "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 101.477142\n",
      "Train Epoch: 12 [19200/50000 (38%)]\tLoss: 97.269356\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 91.724884\n",
      "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 95.955208\n",
      "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 98.364746\n",
      "Train Epoch: 12 [44800/50000 (90%)]\tLoss: 94.519653\n",
      "====> Epoch: 12 Average loss: 96.0099\n",
      "====> Test set loss: 96.2522\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 98.822800\n",
      "Train Epoch: 13 [6400/50000 (13%)]\tLoss: 90.594612\n",
      "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 95.393944\n",
      "Train Epoch: 13 [19200/50000 (38%)]\tLoss: 90.896820\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 91.834290\n",
      "Train Epoch: 13 [32000/50000 (64%)]\tLoss: 93.057938\n",
      "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 96.726372\n",
      "Train Epoch: 13 [44800/50000 (90%)]\tLoss: 91.324745\n",
      "====> Epoch: 13 Average loss: 95.5364\n",
      "====> Test set loss: 96.0421\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 90.067001\n",
      "Train Epoch: 14 [6400/50000 (13%)]\tLoss: 95.613388\n",
      "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 93.507645\n",
      "Train Epoch: 14 [19200/50000 (38%)]\tLoss: 94.342255\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 90.237907\n",
      "Train Epoch: 14 [32000/50000 (64%)]\tLoss: 96.145187\n",
      "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 99.102325\n",
      "Train Epoch: 14 [44800/50000 (90%)]\tLoss: 96.119957\n",
      "====> Epoch: 14 Average loss: 95.2057\n",
      "====> Test set loss: 95.5250\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 93.029419\n",
      "Train Epoch: 15 [6400/50000 (13%)]\tLoss: 94.693123\n",
      "Train Epoch: 15 [12800/50000 (26%)]\tLoss: 94.916336\n",
      "Train Epoch: 15 [19200/50000 (38%)]\tLoss: 98.539711\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 91.419083\n",
      "Train Epoch: 15 [32000/50000 (64%)]\tLoss: 96.450897\n",
      "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 94.971054\n",
      "Train Epoch: 15 [44800/50000 (90%)]\tLoss: 96.150482\n",
      "====> Epoch: 15 Average loss: 94.7988\n",
      "====> Test set loss: 95.2293\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 100.033318\n",
      "Train Epoch: 16 [6400/50000 (13%)]\tLoss: 92.304199\n",
      "Train Epoch: 16 [12800/50000 (26%)]\tLoss: 93.988480\n",
      "Train Epoch: 16 [19200/50000 (38%)]\tLoss: 97.394806\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 100.796921\n",
      "Train Epoch: 16 [32000/50000 (64%)]\tLoss: 97.502518\n",
      "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 90.094810\n",
      "Train Epoch: 16 [44800/50000 (90%)]\tLoss: 98.751633\n",
      "====> Epoch: 16 Average loss: 94.5158\n",
      "====> Test set loss: 95.0400\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 94.858894\n",
      "Train Epoch: 17 [6400/50000 (13%)]\tLoss: 92.867844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [12800/50000 (26%)]\tLoss: 102.144516\n",
      "Train Epoch: 17 [19200/50000 (38%)]\tLoss: 93.785812\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 95.418083\n",
      "Train Epoch: 17 [32000/50000 (64%)]\tLoss: 91.882706\n",
      "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 94.817940\n",
      "Train Epoch: 17 [44800/50000 (90%)]\tLoss: 91.646324\n",
      "====> Epoch: 17 Average loss: 94.2574\n",
      "====> Test set loss: 94.8378\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 96.356934\n",
      "Train Epoch: 18 [6400/50000 (13%)]\tLoss: 92.918198\n",
      "Train Epoch: 18 [12800/50000 (26%)]\tLoss: 92.706802\n",
      "Train Epoch: 18 [19200/50000 (38%)]\tLoss: 100.813385\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 96.421371\n",
      "Train Epoch: 18 [32000/50000 (64%)]\tLoss: 97.982330\n",
      "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 101.981293\n",
      "Train Epoch: 18 [44800/50000 (90%)]\tLoss: 93.683815\n",
      "====> Epoch: 18 Average loss: 94.0198\n",
      "====> Test set loss: 94.5863\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 93.741051\n",
      "Train Epoch: 19 [6400/50000 (13%)]\tLoss: 90.401367\n",
      "Train Epoch: 19 [12800/50000 (26%)]\tLoss: 90.165665\n",
      "Train Epoch: 19 [19200/50000 (38%)]\tLoss: 92.150665\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 94.074242\n",
      "Train Epoch: 19 [32000/50000 (64%)]\tLoss: 95.250122\n",
      "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 95.265427\n",
      "Train Epoch: 19 [44800/50000 (90%)]\tLoss: 89.637924\n",
      "====> Epoch: 19 Average loss: 93.6923\n",
      "====> Test set loss: 94.1354\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    train_vae(epoch, train_loader)\n",
    "    test_vae(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'vae_elbo.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_sample_logprobability(z, mu, logvar):\n",
    "    k = len(logvar)\n",
    "    var = torch.exp(logvar)\n",
    "    log_exp = (-0.5*((z - mu).pow(2) * var.pow(-1)).sum())\n",
    "    return log_exp - 0.5 * k * np.log(2 * np.pi) - torch.log(var).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "norm = scipy.stats.norm(0, 0.1)\n",
    "norm.pdf(0)\n",
    "torch.exp(gaussian_sample_logprobability(torch.zeros(1),torch.zeros(1),torch.zeros(1).fill_(np.log(0.1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu torch.Size([64, 200, 100])\n",
      "eps torch.Size([64, 200, 100])\n",
      "std torch.Size([64, 200, 100])\n",
      "x torch.Size([64, 1, 28, 28])\n",
      "z torch.Size([64, 200, 100])\n"
     ]
    }
   ],
   "source": [
    "# x has shape (M, D)\n",
    "# z has shape (M, K, L)\n",
    "# Where M is mini-batch size, D is the dimension of the input and K is importance sampling count, and L of the hidden layer\n",
    "def importance_sampling_loss_function(model, x, z):\n",
    "    # log sum_k exp (log p(x|z) + log p(z) - log q(z|x)) - log k\n",
    "    losses = []\n",
    "    K = z.shape[1]\n",
    "    l = z.shape[2]\n",
    "    log_k = np.log(K)\n",
    "    mu_0 = torch.zeros((l), requires_grad=False)\n",
    "    logvar_0 = torch.zeros((l), requires_grad=False)\n",
    "    print(\"x\", x.shape)\n",
    "    print(\"z\", z.shape)\n",
    "    for x_i, z_i in zip(x, z):\n",
    "        mu, logvar = model.encode(x)\n",
    "        logp_k = []\n",
    "        for z_ik in z_i:\n",
    "            z_ik = z_ik.unsqueeze(0)\n",
    "            log_pxz = torch.log(model.decode(z_ik))\n",
    "            log_pz = gaussian_sample_logprobability(z_ik, mu_0, logvar_0)\n",
    "            log_qzx = gaussian_sample_logprobability(z_ik, mu, logvar)\n",
    "            #print(f\"log_pxz : {log_pxz[0,0,0,0]}\" )\n",
    "            #print(f\"log_pz : {log_pz}\" )\n",
    "            #print(f\"log_qzx : {log_qzx}\" )\n",
    "            new_term = log_pxz + log_pz - log_qzx\n",
    "            #print(new_term)\n",
    "            logp_k.append(new_term)\n",
    "        logp_k = torch.cat(logp_k)\n",
    "        max_logp_k = torch.max(logp_k)\n",
    "        safe_sum_k = max_logp_k + torch.log(torch.exp(logp_k-max_logp_k).sum())\n",
    "        L = safe_sum_k - log_k\n",
    "        #print(L)\n",
    "        losses.append(L)\n",
    "    return losses\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate_loglikelihood(model, x, K=200):\n",
    "    mu, logvar = model.encode(x)\n",
    "    mu = torch.unsqueeze(mu, 1)\n",
    "    mu = mu.repeat(1, K, 1)\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    std = torch.unsqueeze(std, 1)\n",
    "    std = std.repeat(1, K, 1)\n",
    "    eps = torch.randn_like(std)\n",
    "    print(\"mu\", mu.size())\n",
    "    print(\"eps\", eps.size())\n",
    "    print(\"std\", std.size())\n",
    "    z = mu + eps * std\n",
    "    losses = importance_sampling_loss_function(model, x, z)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    list_of_loglikelihood = []\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        loglikelihood = evaluate_loglikelihood(model, data)\n",
    "        print(loglikelihood)\n",
    "        list_of_loglikelihood += loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
